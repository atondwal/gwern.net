---
title: Tool AIs Want to Be Agent AIs
description: Tool AIs limited purely to inferential tasks will be less intelligent, efficient, and economically valuable than reinforcement-learning AIs learning actions over computation/data/training/architecture/hyperparameters/external-resource use.
created: 7 Sep 2016
status: in progress
tags: decision theory, statistics, NN, computer science, transhumanism
...

> Autonomous AI systems (Agent AIs) trained using [reinforcement learning](!Wikipedia) can do harm when they take wrong actions, especially superintelligent Agent AIs. One solution would be to eliminate their agency by not giving AIs the ability to take actions, confining them to purely informational or inferential tasks such as classification or prediction (Tool AIs), and have all actions be approved & executed by humans, giving equivalently superintelligent results without the risk. I argue that this is not an effective solution for two major reasons. First, because Agent AIs will economically outcompete Tool AIs and humans will inevitably use Agent AIs as much as possible. Secondly, because Agent AIs will be better at inferential tasks than Tool AIs, and this is inherently due to their greater agency: the same algorithms which learn how to perform actions can be used to select important datapoints to learn inference over, how long to learn, how to more efficiently execute inference, how to design themselves, how to optimize hyperparameters, how to make use of external resources such as long-term memories or external software or large databases or the Internet, and how best to acquire new data. All of these actions will result in Agent AIs more intelligent than Tool AIs, in addition to their greater economic competitiveness. Thus, Tool AIs are inferior on every dimension to Agent AIs, and use of Tool AIs is a highly unstable equilibrium.

# Background

One proposed solution to AI risk is to suggest that AIs could be limited purely to supervised/unsupervised learning, and not given access to any sort of capability that can directly affect the outside world such as robotic arms.
In this framework, AIs are treated purely as mathematical functions mapping data to an output such as a classification probability, similar to a logistic or linear model but far more complex; most deep learning neural networks like ImageNet image classification convolutional neural networks (CNN)s would qualify.
The gains from AI then come from training the AI and then asking it many questions which humans then review & implement in the real world as desired.
So an AI might be trained on a large dataset of chemical structures labeled by whether they turned out to be a useful drug in humans and asked to classify new chemical structures as useful or non-useful; then doctors would run the actual medical trials on the drug candidates and decide whether to use them in patients etc.
Or an AI might look like Google Maps/[Waze](!Wikipedia): it answers your questions about how best to drive places better than any human could, but it does not control any traffic lights country-wide to optimize traffic flows nor will it run a self-driving car to get you there.
This theoretically avoids any possible runaway of AIs into malignant or uncaring actors who harm humanity by satisfying dangerous utility functions and developing instrumental drives.
After all, if they can't take any actions, how can they do anything that humans do not approve of?

Two variations on this limiting or boxing theme are

- [Oracle AI](https://wiki.lesswrong.com/wiki/Oracle_AI)
- [Tool AI](https://wiki.lesswrong.com/wiki/Tool_AI)

Oracle AIs remain a hypothetical because it's unclear how to write such utility functions.
The second approach, 'Tool AI', is just an extrapolation of current systems but has two basic problems.

# Economic

First and most commonly pointed out, agent AIs are more economically competitive as they can fully replace humans 'in the loop'.
In any sort of process, [Amdahl's law](!Wikipedia) notes that as steps get optimized, the optimization does less and less as the output becomes dominated by the slowest step - if a step only takes 10% of the time or resources, then even infinite optimization of that step down to zero time/resources means that the output will increase by no more than 10%.
So if a human overseeing a, say, high-frequency trading (HFT) algorithm, accounts for 50% of the latency in decisions, then the HFT algorithm will never run more than twice as fast as it does now, which is a crippling disadvantage.
(Hence, the [Knight Capital](!Wikipedia "Knight Capital Group#2012 stock trading disruption") debacle is not too surprising - no profitable HFT firm could afford to put too many humans into its loops, so when something does go wrong, it can be difficult for humans to figure out the problem & intervene before the losses mount.)
As the AI gets better, the gain from replacing the human increases greatly, and may well justify replacing them with an AI inferior in many other respects but superior in some key aspect like cost or speed.
This could also apply to error rates - in airline accidents, human error now causes the overwhelming majority of accidents due to their presence as overseers of the [autopilots](!Wikipedia) and it's unclear that a human pilot represents a net safety gain; [and in 'advanced chess'](/Notes#advanced-chess-obituary), grandmasters initially chose most moves and used the chess AI for checking for tactical errors and blunders, which transitioned through the late '90s and early '00s to human players (not even grandmasters) turning over most playing to the chess AI but contributing a great deal of win performance by picking & choosing which of several AI-suggested moves to use, but as the chess AIs improved, at some point around 2007 victories increasingly came from the *humans* making mistakes which the opposing chess AI could exploit, even mistakes as trivial as 'misclicks' (on the computer screen), and now in advanced chess, human contribution has decreased to largely preparing the chess AIs' opening books & looking for novel opening moves which their chess AI can be better prepared for.

At some point, there is not much point to keeping the human in the loop at all since they have little ability to check the AI choices and become 'deskilled' (think [drivers following GPS](!Wikipedia "Death by GPS") [directions](http://ideas.4brad.com/what-if-city-ran-waze-and-you-had-obey-it-could-cure-congestion)), correcting less than they screw up.
(Hence the old joke: "the factory of the future will be run by a man and a dog; the dog will be there to keep the man away from the factory controls.")
For a successful autonomous program, just keeping up with growth alone makes it difficult to keep humans in the loop; the US drone warfare program has become such a central tool of US warfare that the US Air Force finds it extremely difficult to hire & retain enough human pilots overseeing its drones, and there are indications that operational pressures are slowly eroding the human control & turning them into rubberstamps, and for all its protestations that it would always keep a human in the decision-making loop, the Pentagon is, unsurprisingly, inevitably, sliding towards fully autonomous drone warfare as the next technological step to maintain military superiority over Russia & China.
(See ["Meet The New Mavericks: An Inside Look At America's Drone Training Program"](https://www.fastcompany.com/3054521/meet-the-new-mavericks-an-inside-look-at-americas-drone-training-program "We traveled to Holloman Air Force Base for a glimpse of the future of war-and the future of work"); ["Future is assured for death-dealing, life-saving drones"](http://www.guardian.co.uk/world/2012/aug/04/future-drones "Developers predict that pilotless devices will join planes in civilian airspace - and dream of electric robots counting sheep"); ["Sam Altman's Manifest Destiny"](http://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny "Is the head of Y Combinator fixing the world, or trying to take over Silicon Valley?"); ["The Pentagon's 'Terminator Conundrum': Robots That Could Kill on Their Own"](http://www.nytimes.com/2016/10/26/us/pentagon-artificial-intelligence-terminator.html "The United States has put artificial intelligence at the center of its defense strategy, with weapons that can identify targets and make decisions."); ["Attack of the Killer Robots"](https://www.buzzfeed.com/sarahatopol/how-to-save-mankind-from-the-new-breed-of-killer-robots "Forget about drones, forget about dystopian sci-fi â€” a terrifying new generation of autonomous weapons is already here. Meet the small band of dedicated optimists battling nefarious governments and bureaucratic tedium to stop the proliferation of killer robots and, just maybe, save humanity from itself."))

Fundamentally, autonomous agent AIs are what we and the free market *want*; everything else is a surrogate or irrelevant loss function.
We don't want low log-loss error on ImageNet, we want to refind a particular personal photo; we don't want excellent advice on which stock to buy for a few microseconds, we want a money pump spitting cash at us; we don't want a drone to tell us where Osama bin Laden was an hour away, we want it to have blown him up when it saw him; etc.
Idiosyncratic situations, legal regulation, fears of tail risks from very bad situations, worries about correlated or systematic failures (like hacking a drone fleet), and so on may slow or stop the adoption of Agent AIs, but the pressure will always be there.

So for this reason alone, we expect to see Agent AIs to systematically be preferred over Tool AIs as soon as either one becomes useful.

# Intelligence

Agent AIs will be chosen over Tool AIs  - for reasons aside from not being what anyone wants and something that will be severely penalized by free markets or simply there being multiple agents choosing whether to use a Tool AI or an Agent AI in any kind of competitive scenario -  also suffer from the problem that *the best Tool AI's performance/intelligence will be equal to or worse than the best Agent AI, probably worse, and possibly much worse.*

An Agent AI has the potential, often realized in practice, to outperform any Tool AI: it can get better results with less computation, less data, less manual design, less post-processing of its outputs, on harder domains.

(Trivial proof: Agent AIs are supersets of Tool AIs - an Agent AI, by not taking any actions besides communication or random choice, can reduce itself to a Tool AI; so in cases where actions are unhelpful, it performs the same as the Tool AI, and when actions can help, it can perform better; hence, an Agent AI can always match or exceed a Tool AI.
At least, assuming sufficient data that in the environments where actions are not helpful, it can learn to stop acting, and in the ones where they are, it has a distant enough horizon to pay for the exploration.)

More seriously, not all data is created equal.
Not all data points are equally valuable to learn from, require equal amounts of computation, should be treated identically, should inspire identical followup data sampling, or actions.
Inference and learning can be *much* more efficient if the algorithm can choose how to compute on what data with which actions.

There is no hard Cartesian boundary [between an algorithm & its environment](!Wikipedia "Extended mind thesis") such that control of the environment is irrelevant to the algorithm and vice-versa and its computation can be carried out without regard to the environment - there are simply many layers between the core of the algorithm and the furthest part of the environment, and the more layers that the algorithm can model & control, the more it can do.
(Indeed, while Google Maps was used as a paradigmatic example of a Tool AI, it's not clear how hard this can be pushed: Google Maps/Waze is, of course, running extensive A/B tests on its users, and to the extent that users make any use of the information and increase/decrease their use of Google Maps, which many will and will do so blindly, Google Maps will get feedback after changing the real world...)

This is a highly general point which can be applied on many levels.
This point often arises in classical statistics/[experimental design](!Wikipedia)/decision theory where adaptive techniques can greatly outperform fixed-sample techniques for both inference and actions/losses: an [sequential analysis](!Wikipedia) trial testing a hypothesis can often terminate after a fraction of the equivalent fixed-sample trial's sample size (and/or loss) while [exploring multiple questions](!Wikipedia "Response surface methodology"); an [adaptive](!Wikipedia "Adaptive clinical trial") [multi-armed bandit](!Wikipedia) will have much lower regret than any non-adaptive solution, but it will also be inferentially better at estimating which arm is best and what the performance of that arm is (see the 'best-arm problem': [Bubeck et al 2009](https://arxiv.org/pdf/0802.2655.pdf "Pure exploration in multi-armed bandits problems"), [Audibert et al 2010](https://hal.inria.fr/file/index/docid/654404/filename/COLT10.pdf "Best Arm Identification in Multi-Armed Bandits"), [Gabillon et al 2011](https://papers.nips.cc/paper/4478-multi-bandit-best-arm-identification.pdf "Multi-Bandit Best Arm Identification"), [Mellor 2014](https://www.escholar.manchester.ac.uk/api/datastream?publicationPid=uk-ac-man-scw:227658&datastreamId=FULL-TEXT.PDF "Decision Making Using Thompson Sampling"), [Jamieson & Nowak 2014](http://nowak.ece.wisc.edu/bestArmSurvey.pdf "Best-arm Identification Algorithms for Multi-Armed Bandits in the Fixed Confidence Setting"), [Kaufmann et al 2014](http://arxiv.org/pdf/1407.4443v1.pdf "On the Complexity of Best Arm Identification in Multi-Armed Bandit Models")), and an adaptive [optimal design](!Wikipedia) can constant-factor (gains of 50% or more are possible compared to naive designs like even allocation; [McClelland 1997](http://www2.psych.ubc.ca/~schaller/528Readings/McClelland1997.pdf "Optimal design in psychological research")) minimize total [variance](!Wikipedia) by focusing on unexpectedly difficult-to-estimate arms (while a fixed-sample trial can be seen as ideal for when one values precise estimates of all arms equally and they have equal variance, which is usually not the case); even a [Latin square](!Wikipedia) or [blocking](!Wikipedia "Randomized block design") or [rerandomization](https://arxiv.org/pdf/1207.5625.pdf "'Rerandomization to improve covariate balance in experiments', Morgan & Rubin 2012") design rather than simple randomization can be seen as reflecting this benefit (avoiding the potential for imbalance in allocation across arms by deciding in advance the sequence of 'actions' taken in collecting samples).

The wide variety of uses of action is a major theme in recent work in  AI (specifically, [deep learning](!Wikipedia)/neural networks) research and increasingly key to achieving the best performance on inferential tasks as well as reinforcement learning/optimization/agent-y tasks.
Roughly, we can try to categorize them by the 'level' of the NN they work on.

There are:

1. actions internal to a computation:

    - inputs
    - intermediate states
    - accessing the external 'environment'
    - amount of computation
    - enforcing constraints/finetuning quality of output
    - changing the loss function applied to output
2. actions internal to training the NN:

    - the gradient itself
    - size & direction of gradient descent steps on each parameter
    - overall gradient descent learning rate and learning rate schedule
    - choice of data samples to train on
3. internal to the NN design step

    - hyperparameter optimization
    - NN architecture
4. internal to the dataset

    - active learning
    - optimal experiment design
5. internal to interaction with environment

    - adaptive experiment / multi-armed bandit / exploration for reinforcement learning

So to put it concretely: CNNs with adaptive computations will be computationally faster for a given accuracy rate than fixed-iteration CNNs, CNNs with attention classify better than CNNs without attention, CNNs with focus over their entire dataset will learn better than CNNs which only get fed random images, CNNs which can ask for specific kinds of images do better than those querying their dataset, CNNs which can trawl through Google Images and locate the most informative one will do better still, CNNs which access rewards from their user about whether the result was useful will deliver more relevant results, CNNs whose hyperparameters are automatically optimized by an RL algorithm will perform better than CNNs with handwritten hyperparameters, CNNs whose architecture as well as standard hyperparameters are designed by RL agents will perform better than handwritten CNNs... and so on.
(It's actions all the way down.)
The drawback to all this is the implementation difficulty is higher, the sample efficiency can be better or worse (individual parts will have greater sample-efficiency but data will be used up training the additional flexibility of other parts), and the computation requirements for training can be much higher; but the ultimate performance is better, and the gap probably grows as GPUs & datasets get bigger and tasks get more difficult & valuable in the real world.

## Actions internal to a computation

Inside a specific NN, while computing the output for an input question, a NN can make choices about how to handle it.

It can choose what parts of the input to run most of its computations on, while throwing away or computing less on other parts of the input, which are less relevant to the output, using "attention mechanisms".
Attention mechanisms are responsible for many increases in performance, but especially improvements in RNNs' ability to do sequence-to-sequence translation by revisiting important parts of the sequence, and in CNNs' ability to recognize images by focusing on ambiguous or small parts of the image.
The bidirectional RNN also often used in natural language translation doesn't explicitly use attention mechanisms but is believed to help by giving the RNN a second look at the sequence.
Indeed, so universal that it often goes without mention is that the [LSTM](!Wikipedia "Long short-term memory")/GRU mechanism which improves almost all RNNs is itself a kind of attention mechanism: the LSTM cells learn which parts of the hidden state/history are important and should be kept, and whether and when the memories should be forgotten and fresh memories loaded into the LSTM cells.

http://distill.pub/2016/augmented-rnns/
["Foveation-based Mechanisms Alleviate Adversarial Examples"](http://arxiv.org/abs/1511.06292), Luo et al 2016;
"Modeling Human Reading with Neural Attention" Hahn & Keller 2016 https://arxiv.org/pdf/1608.05604.pdf
"Hierarchical Object Detection with Deep Reinforcement Learning" Bellver et al 2016 https://imatge-upc.github.io/detection-2016-nipsws/

- computation on intermediates
REINFORCE neural Turing machine "Hybrid computing using a neural network with dynamic external memory", Graves et al 2016 https://www.gwern.net/docs/2016-graves.pdf https://deepmind.com/blog/differentiable-neural-computers/ ; ["Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes"](https://arxiv.org/abs/1610.09027), Rae et al 2016
database queries "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning" Narasimhan et al 2016 https://arxiv.org/pdf/1603.07954.pdf
adaptive computation: not just choosing which parts of data to process multiple times but how many times to process - spend more computation on hard parts of problem "Adaptive Computation Time for Recurrent Neural Networks" Graves 2016 https://arxiv.org/abs/1603.08983 "Spatially Adaptive Computation Time for Residual Networks", Figurnov et al 2016 https://arxiv.org/pdf/1612.02297v1.pdf
- quality of output like global constraints, finetuning supervised learning results, more user-relevant reward functions than the usual loss functions
AlphaGo ["Mastering the game of Go with deep neural networks and tree search"](https://www.researchgate.net/profile/Timothy_Lillicrap/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search/links/56a90f7d08ae7f592f0d5d0c.pdf), Silver et al 2016 ; "Deep Reinforcement Learning for Dialogue Generation", Li et al 2016 https://arxiv.org/pdf/1606.01541.pdf
"Reward Augmented Maximum Likelihood for Neural Structured Prediction", Norouzi et al 2016 http://papers.nips.cc/paper/6547-reward-augmented-maximum-likelihood-for-neural-structured-prediction
"Tuning Recurrent Neural Networks with Reinforcement Learning [Note-RNN]", Jaques et al 2016 https://openreview.net/pdf?id=BJ8fyHceg
"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", Wu et al 2016 https://arxiv.org/abs/1609.08144 https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
"Sequence level training with recurrent neural networks", Ranzato et al 2016 https://arxiv.org/abs/1511.06732
"Deep Reinforcement Learning for Mention-Ranking Coreference Models", Clark & Manning 2016 https://arxiv.org/pdf/1609.08667.pdf
compression "Language as a Latent Variable: Discrete Generative Models for Sentence Compression" Miao & Blunsom 2016 https://arxiv.org/pdf/1609.07317.pdf

loss/training functions: GANs and training very large networks: actor-critic RL methods - ["Decoupled Neural Interfaces using Synthetic Gradients"](https://arxiv.org/abs/1608.05343) http://cnichkawde.github.io/SyntheticGradients.html


2. internal to training:

gradient descent "Learning to learn by gradient descent by gradient descent", Andrychowicz et al 2016 https://arxiv.org/abs/1606.04474
"Deep Reinforcement Learning for Accelerating the Convergence Rate", Fu et al 2016 https://openreview.net/pdf?id=Syg_lYixe
learning rates for gradient descent http://openreview.net/pdf?id=Sy7m72Ogg "An Actor-Critic Algorithm for Learning Rate Learning" Xu et al 2016
RL: prioritized traces, prioritized replay
boosting, SGD hard-negative mining and prioritizing hard samples "Neural Data Filter for Bootstrapping Stochastic Gradient Descent" Fan et al 2016 http://openreview.net/forum?id=SyJNmVqgg
even errors in training (synthetic gradients == actor-critic)

2. internal to NN design:

hyperparameter optimization: Gaussian processes, RNNs "Neural architecture search with reinforcement learning", Zoph & Le 2016 http://openreview.net/pdf?id=r1Ue8Hcxg https://www.reddit.com/r/MachineLearning/comments/5b5022/r_neural_architecture_search_with_reinforcement/
"Designing Neural Network Architectures using Reinforcement Learning", Baker et al 2016 https://arxiv.org/abs/1611.02167
"Learning to Learn for Global Optimization of Black Box Functions", Chen et al 2016 https://arxiv.org/pdf/1611.03824.pdf
"RL^2^: Fast Reinforcement Learning via Slow Reinforcement Learning", Duan et al 2016 https://openreview.net/pdf?id=HkLXCE9lx
- ["Learning to reinforcement learn"](https://arxiv.org/abs/1611.05763), Wang et al 2016

3. actions internal to data selection:

class imbalance
active learning: babies ask parents for only a few labels to finetune unsupervised learning but still learn language
["Active Learning Literature Survey"](http://burrsettles.com/pub/settles.activelearning.pdf) (Settles 2010) discusses the practical advantages to machine learning algorithms of careful choice of data points to learn from or 'label', and gives some of the known theoretical results on how large the benefits are - on a toy problem, an error rate _e_ decreasing in sample count from $\mathcal{O}(\frac{1}{\epsilon})$ to $\mathcal{O}(log(\frac{1}{\epsilon}))$, or in a Bayesian setting, a decrease of $\mathcal{O}(\frac{d}{\epsilon})$ to $\mathcal{O}(d \cdot log(\frac{1}{\epsilon}))$.

"Active Learning for High Dimensional Inputs using Bayesian Convolutional Neural Networks", Islam 2016 https://github.com/Riashat/Active-Learning-Bayesian-Convolutional-Neural-Networks/raw/master/Presentations/Thesis/Islam%20Riashat%20MPhil%20MLSALT%20Thesis.pdf ; "Uncertainty in Deep Learning", Gal 2016 / http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf "Bayesian Active Learning for Classification and Preference Learning", Houlsby et al 2011  https://arxiv.org/abs/1112.5745
optimal experiment

4. external actions

adaptive experiments
MABs
RL

20:56 <@gwern> to get better performance on a variety of tasks, handing it more actions and options will often be found to be helpful as it can go after the data which will be most useful
20:58 <@gwern> that's my point. optimizing explorations can often lead to prediction/classification/inference gains
20:58 <@gwern> there's no hard and fast separation from RL stuff and supervised learning. agents benefit from better inference, but inference also benefits from agentiness
20:59 <@gwern> think about just the simplest multi-armed bandit model. by letting the algorithm decide where to sample, you can get better inference about which is the optimal arm and what its payoff is, entirely separate from issues of maximizing reward
21:02 <@gwern> nshepperd: or you could just do a fixed-sample trial and leave any action step outside of the algorithm. the MAB will still inferentially outperform the fixed-sample trial


Why does treating all these levels as decision or reinforcement learning problems help so much?

One answer is that most points are not near any decision boundary, or are highly predictable and contribute little information.
These points need not be computed extensively, nor trained on much, nor collected further.
If a particular combination of variables is already being predicted with high accuracy (perhaps because it's common), adding even an infinite number of additional samples will do little; one sample from an unsampled region far away from the previous samples may be dramatically informative.
You need the *right* data, not more data.

Another answer is the "curse of dimensionality": in many environments, the tree of possible actions and subsequent rewards grows exponentially, so any sequence of actions over more than a few timesteps is increasingly unlikely to ever be sampled.
A dataset of randomly-generated sequences of robot arm movements intended to grip an object would likely include no rewards (successful grips) at all, because it requires a long sequence of finely calibrated arm movements; with no successes, how could the tool AI learn to manipulate an arm?
It must be able to make progress by testing its best arm movement sequence candidate, then learn from that and test the better arm movement, and so on, until it succeeds.
Or imagine training a Go program by creating a large dataset of randomly-generated Go boards, then evaluating each possible move's value by playing out a game between random agents from it; this would not work nearly as well as training on actual human-generated board positions which target the vanishingly small set of high-quality games & moves.
The exploration homes in on the exponentially shrinking optimal area of the movement tree based on its current knowledge, discarding the enormous space of bad possible moves.
In contrast, a tool AI cannot lift itself up by its bootstraps. It merely gives its best guess on the static current dataset, and that's that. If you don't like the results, you can gather more data, but it probably won't help that much because you'll give it more of what it already has.



random Go boards
random actions in a robot arm
sparse rewards
exponentially difficult to reach rewards with actions ("39. Re graphics: A picture is worth 10K words - but only those to describe the picture. Hardly any sets of 10K words can be adequately described with pictures.")
== rewards not observed in a dataset
_Montezuma's Revenge_: epsilon-greedy vs density-estimation ("Unifying Count-Based Exploration and Intrinsic Motivation", Bellemare et al 2016 https://arxiv.org/pdf/1606.01868.pdf )

supervised/unsupervised ML algorithm which computes a function but takes no actions
contrast to reinforcement learning and automated systems

turn around Karnofsky's example: Google Maps + Waze: Waze is used to tell drivers where to go, and drivers produce information/metadata about road conditions, all of which is constantly being optimized by things like A/B tests to make drivers more likely to use Waze and go where Waze tells them to go... Waze does nothing on its own, but the tool AI has taken on aspects of an agent AI

11:59 <@gwern> http://arxiv.org/abs/1511.02793 more gains from adding reinforcement-learning to tool AIs?

actions can serve as a form of supervision - in DQN what would you 'label' each Atari screen as...?

17:16 <@gwern> 'One question I remember came from Tieleman. He asked the panelists about their opinions on active learning/exploration as an option for efficient unsupervised learning. Schmidhuber and Murphy responded, and before I reveal their response, I really liked it. In short (or as much as I'm certain about my memory,) active exploration will happen naturally as the consequence of rewarding better explanation of the world. Knowledge of the surrounding world and its accumulation should be rewarded, and to maximize this reward, an agent or an algorithm will active explore the surrounding area (even without supervision.) According to Murphy, this may reflect how babies learn so quickly without much supervising signal or even without much unsupervised signal (their way of active exploration compensates the lack of unsupervised examples by allowing a baby to collect high quality unsupervised examples.)' http://www.kyunghyuncho.me/home/blog/briefsummaryofthepaneldiscussionatdlworkshopicml2015 so they think tool AIs will inevitably segue into agent AIs...
