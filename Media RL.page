---
title:
description:
tags: statistics, decision theory
created: 24 Dec 2016
status: notes
belief: possible
...


When you decide to watch a movie, it can be tough to pick.
Do you pick a new movie or a classic you watched before & liked?
If the former, how do you pick from all the thousands of plausible unwatched candidate movies; and if the former, how soon is too soon to rewatch?

I tend to default to a new movie, reasoning that I might really like it and discover a new classic to add to my library.
Once in a while, I rewatch some movie I really liked, and I like it almost as much as the first time, and I think to myself, "why did I wait 15 years to rewatch this, why didn't I watch this last week instead of movie _X_ which was mediocre, or _Y_ before that which was crap? I'd forgotten most of the details, and it wasn't boring at all! I should rewatch movies more often."
(Then of course I don't because I think "I should watch _Z_ to see if I like it...")
Maybe many other people do this too, judging from how often I see people mentioning watching a new movie and how rare it is for someone to mention rewatching a movie; it seems like people predominantly (maybe 80%+ of the time) watch new movies rather than rewatch a favorite.

The tricky thing is that each watch of a movie decreases the value of another watch (diminishing marginal value), but in a time-dependent way: 1 day is usually much too short and the value may even be negative, but 1 decade may be too long - the movie's entertainment value 'recovers' slowly and smoothly over time.

This sounds like a classic reinforcement learning (RL) exploration-exploitation tradeoff problem: we don't want to watch only new movies, because the average new movie is mediocre, but if we watch only known-good movies, then we miss out on all the good movies we haven't seen and fatigue may make watching the known-good ones downright unpleasant.

One could imagine some simple heuristics, such as setting a cutoff for 'good' movies and then alternate between watching whatever new movie sounds the best (and adding it to the good list if it is better than the cutoff) and watching the oldest unwatched good movie.
This seems suboptimal because in a typical RL problem, exploration will decrease over time as most of the good decisions become known and it becomes more important to benefit from them than to keep trying new options, hoping to find better ones; one might explore using 100% of one's decisions at the beginning but steadily decrease the exploration rate down to a fraction of a percent towards the end - in few problems is it optimal to keep eternally exploring on, say, 80% of one's decisions.
Eternally exploring on the majority of decisions would only make sense in an extremely unstable environment where the best decision constantly rapidly changes; this, however, doesn't seem like the movie-watching problem, where typically if one really enjoyed a movie 1 year ago, one will almost always enjoy it now too.

Better RL algorithms exist, assuming one has a good model of the problem/environment, such as Thompson sampling.
This minimizes our regret in the long run, by estimating the probability of being able to find an improvement and decreasing exploration as the probability of improvements decreases.
The real question is the exact model.

I would start here with movie ratings. A movie gets rated 1-10, and we want to maximize the sum of ratings over time; we can't do this simply by picking the highest-ever rated movie, because once we watch it, it suddenly stops being so enjoyable; so we need to model some sort of drop.
A simple parametric model would to treat it as something like an exponential curve over time: gradually increasing and approaching the original rating but never reaching it (the magic of the first viewing can never be recaptured).
So each viewing might drop the rating by a certain number _v_ and then the exponential curve increases by _r_ units per day - intuitively, I would say that on a 10-point scale, a viewing drops an immediate rewatch by at least 2 points, and then it takes ~5 years to almost fully recover within +-0.10 points, so we would initially assign priors centered on _v_=2 and _r_= `(2-0.10) / (365*5) ~= 0.001` and then our model should finetune those rough estimates based on the data (I would guess it takes less than 5 years to recover rather than more, so this estimate would bias towards new movies/exploration).
